
# ++++++++++++++++++++++++++++++++++++++++

# General Parameters, see comment for each definition
# choose the booster, can be gbtree or gblinear
booster = gbtree
# choose logistic regression loss function for binary classification
objective = binary:logistic


# +++++++++++++++++++++++++++++++++++++++

# Tree Booster Parameters
# step size shrinkage
eta = 1.0
# minimum loss reduction required to make a further partition
gamma = 1.0
# minimum sum of instance weight(hessian) needed in a child
min_child_weight = 1
# maximum depth of a tree
max_depth = 3

# L2 regularization term on weights, increase this value will make model more conservative.
lambda=1

# L1 regularization term on weights, increase this value will make model more conservative.
alpha=0

# ++++++++++++++++++++++++++++++++++++++++++++++

# Task Parameters
# the number of round to do boosting
num_round = 10
# 0 means do not save any model except the final round model
save_period = 1
# The path of training data
data = "Ontime_flight_arrivalModified.svmlight.train"
# The path of validation data, used to monitor training process, here [test] sets name of the validation set
eval[test] = "Ontime_flight_arrivalModified.svmlight.test"
# evaluate on training data as well each round
eval_train = 1
# The path of test data
test:data = "Ontime_flight_arrivalModified.svmlight.test"

# ++++++++++++++++++++++++++++++++++++++++++++++

# We can also monitor both training and test statistics, by adding following lines to configure
#eval[test] = "Ontime_flight_arrivalModified.svmlight.test"
#eval[trainname] = "Ontime_flight_arrivalModified.svmlight.train"

# Want to monitor average log-likelihood of each prediction during training
eval_metric=auc
eval_metric=logloss




# How to add new eval_metric: merror,auc,rmse,mae,map

# You will find 0002.model in the current folder. If you want to change the output folder of models, add model_dir=foldername. By default xgboost saves the model of last round.
# model_dir=foldername

